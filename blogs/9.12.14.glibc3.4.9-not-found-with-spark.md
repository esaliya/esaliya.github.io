If you encounter an error similar to the following, which complains that `GLIBCXX_3.4.9` could not be found, while running an application with [Apache Spark](https://spark.apache.org/) you can avoid this by switching [Spark's compression method](https://spark.apache.org/docs/latest/configuration.html#compression-and-serialization) from `snappy` to something such as `lzf`.


    ...
    Caused by: java.lang.UnsatisfiedLinkError: .../snappy-1.0.5.3-1e2f59f6-8ea3-4c03-87fe-dcf4fa75ba6c-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by.../snappy-1.0.5.3-1e2f59f6-8ea3-4c03-87fe-dcf4fa75ba6c-libsnappyjava.so)
    
There are a few ways how one can pass configuration options to Spark. The naive way seems to be through command line as, 

    --conf "spark.io.compression.codec=lzf"

On a side note, you can find what GLIBC versions are available by running `strings /usr/lib/libstdc++.so.6 | grep GLIBC`
    

References

- A [mail thread](http://comments.gmane.org/gmane.comp.lang.scala.spark.user/3212) on this
- Spark [configuration options](https://spark.apache.org/docs/latest/configuration.html#compression-and-serialization)
